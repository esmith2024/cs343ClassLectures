{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation through time\n",
    "\n",
    "## RNN review\n",
    "\n",
    "*A RNN with a single hidden layer with ReLU activation, and an output layer with softmax, is defined by what?*\n",
    "1. $W_{xh}$ = weights on input\n",
    "2. $W_{hh}$ = weights on hidden layer\n",
    "3. $b_{h}$ = weights on hidden layer\n",
    "4. $W_{hq}$= output weights\n",
    "5. $b_{q}$ = output bias\n",
    "\n",
    "In Particualr y_netIn ~ $Wh_{xh}* x_t + W_{hh}h_{t-1} + b_h$\n",
    "\n",
    "and the equivilant of y_netact ~ $\\phi Wh_{xh}* x_t + W_{hh}h_{t-1} + b_h$\n",
    "\n",
    "and the equivilant of z_netIn ~ $W_{hq} * y_{netAct} +b_q$\n",
    "\n",
    "## One more step!\n",
    "\n",
    "*A RNN with two hidden layers, each with ReLU activation, and an output layer with softmax, is defined by what?*\n",
    "1. $W_{xh1}$ = weights on input\n",
    "2. $W_{hh1}$ = weights on hidden layer\n",
    "3. $b_{h1}$ = weights on hidden layer\n",
    "4. $W_{xh2}$ = weights on input\n",
    "5. $W_{hh2}$ = weights on hidden layer\n",
    "6. $b_{h2}$ = weights on hidden layer\n",
    "7. $W_{hq}$= output weights\n",
    "9. $b_{q}$ = output bias\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a MLP\n",
    "\n",
    "A MLP looks quite similar to a RNN. Imagine a MLP with one hidden layer with ReLU activation, and an output layer with softmax. Bring up your implementation of backpropagation from project 2. Here's the forward pass:\n",
    "1. y_netIn = X@y_wts + y_b\n",
    "2. y_netAct = max(y_netIn, 0)\n",
    "3. z_netIn = y_netAct@z_wts + z_b\n",
    "4. z_netAct = 1 / (1 + exp(-z_netIn))\n",
    "5. loss (cross entropy)\n",
    "\n",
    "What are the derivatives you calculated?\n",
    "\n",
    "1. dz_netAct (meets loss from above and z_netAct from below): derivative of crossentropy loss: -1/len(z_netAct)*z_netAct\n",
    "2. Three parts!\n",
    "   * dz_netIn (meets dz_netAct from above and what from below?)\n",
    "      * from below meets the gradient of softmax for z_net_act \n",
    "      * (dz_netAct * one_hot(y,num_outputs)-z_netAct)\n",
    "   * dz_wts (meets dz_netIn from above and y_netAct from below?)\n",
    "      * (dz_netin.T@y_netAct).T + regularization*z_wts\n",
    "   * dz_b (meets dz_netIn from above)\n",
    "      * sum(dz_netIn, axis = 1) \n",
    "3. dy_netAct (meets dz_netIn from above and dy_netIn?): dz_netin @ z_wts.T\n",
    "   * and the z_wts from below\n",
    "4. Three parts!\n",
    "   * dy_netIn (meets dy_netAct from above and y_wts?): \n",
    "      * dy_netIn*(np.where(y_netIn<= 0, 0,1))\n",
    "   * dy_wts\n",
    "      * features times the dy_net_in and regulairzation of the y_wts\n",
    "   * dy_b\n",
    "      *  sum(dy_netIn, axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a RNN\n",
    "\n",
    "Let's stick with a one hidden layer RNN defined as before. Here's the forward pass:\n",
    "\n",
    "1. $h_t = W_{xh} x_t + w_{hh} h_{t-1} + b_h$\n",
    "2. $o_t = W_{qh} h_t + b_q$\n",
    "\n",
    "Loss is $1/T \\sum_{t=1}^T l(o_t, y_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for backprop, we need to calculate:\n",
    "\n",
    "1. $dy$ (analogous to dz_netAct): derivative of Loss * o(outputs) = (-1/len(y_t)*y_t)\n",
    "2. Two parts!\n",
    "   * $dW_{qh}$ (meets dy from above and output from h from below, which we will call h_netAct)\n",
    "      * dy_t@h_netAct_t.T\n",
    "   * $db_{q}$ (meets what dy from above)\n",
    "      * dy_t\n",
    "2. $dh$ (analogous to dy_netAct) W_hq.T@dy_t + h_t, **and then backprop through activation weither tanh or relu to get dhraw_t**\n",
    "3. Three parts!\n",
    "   * $dW_{hh}$ (meets dh from above and $h_{t-1}$ from below)\n",
    "      * $dhraw_t$ @$h_{t-1}$\n",
    "   * $dW_{xh}$ (meets dh from above and $x_{t}$ from below)\n",
    "      * $dh_t$@ $x_t$\n",
    "   * $db_h$ (meets dh from above)\n",
    "      * $dh_t$\n",
    "\n",
    "Take some time and define these."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How would you handle two hidden layers?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with long histories\n",
    "\n",
    "The textbook outlines three strategies:\n",
    "1. Full computation\n",
    "2. Regular truncation\n",
    "3. Random truncation\n",
    "\n",
    "Let's talk about each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karpathy again!\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
